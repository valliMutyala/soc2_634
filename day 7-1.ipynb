{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j95WjI8H-eR0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# Project 2:\n",
        "# Detect flowers based on images\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import style\n",
        "\n",
        "# Model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# preprocess\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.layers import Dropout, Flatten, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from random import shuffle\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDEFCq6p-qvh"
      },
      "outputs": [],
      "source": [
        "X=[]\n",
        "Z=[]\n",
        "IMG_SIZE=150\n",
        "FLOWER_DAISEY_DIR = '/content/gdrive/MyDrive/archive (4)-20240220T012139Z-001/archive (4)/train/daisy'\n",
        "FLOWER_SUNFLOWER_DIR = '/content/gdrive/MyDrive/archive (4)-20240220T012139Z-001/archive (4)/train/sunflower'\n",
        "FLOWER_TULIP_DIR = '/content/gdrive/MyDrive/archive (4)-20240220T012139Z-001/archive (4)/train/tulip'\n",
        "FLOWER_DANDI_DIR = '/content/gdrive/MyDrive/archive (4)-20240220T012139Z-001/archive (4)/train/dandelion'\n",
        "FLOWER_ROSE_DIR = '/content/gdrive/MyDrive/archive (4)-20240220T012139Z-001/archive (4)/train/rose'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8p-IoWV-z2S"
      },
      "outputs": [],
      "source": [
        "def assign_label(img, flower_type):\n",
        "  return flower_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWA4JNS4-0_7"
      },
      "outputs": [],
      "source": [
        "# tqdm --> It creates a progress bar from the loop\n",
        "def make_train_data(flower_type, DIR):\n",
        "  for img in tqdm(os.listdir(DIR)):\n",
        "    label = assign_label(img, flower_type)\n",
        "    path = os.path.join(DIR, img)\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    X.append(np.array(img))\n",
        "    Z.append(str(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "KF6nI3a2-4Ea",
        "outputId": "9329eebe-d527-4441-a5e0-745a4f049be1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/archive (4)-20240220T012139Z-001/archive (4)/train/daisy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-810814b4bde3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Daisy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLOWER_DAISEY_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c8538b1df15c>\u001b[0m in \u001b[0;36mmake_train_data\u001b[0;34m(flower_type, DIR)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tqdm --> It creates a progress bar from the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflower_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflower_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/archive (4)-20240220T012139Z-001/archive (4)/train/daisy'"
          ]
        }
      ],
      "source": [
        "make_train_data('Daisy', FLOWER_DAISEY_DIR)\n",
        "print(len(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWTKjTFd-60P"
      },
      "outputs": [],
      "source": [
        "make_train_data('Sunflower', FLOWER_SUNFLOWER_DIR)\n",
        "print(len(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lky2vG-o-9HY"
      },
      "outputs": [],
      "source": [
        "make_train_data('Daisy', FLOWER_DAISEY_DIR)\n",
        "print(len(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWg0qEjws5c-"
      },
      "outputs": [],
      "source": [
        "\n",
        "make_train_data('dandelion', FLOWER_DANDI_DIR)\n",
        "print(len(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLnyBRaGvo2o"
      },
      "outputs": [],
      "source": [
        "make_train_data('Rose', FLOWER_ROSE_DIR)\n",
        "print(len(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps6biTH0vs7S"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(5, 2)\n",
        "fig.set_size_inches(15, 15)\n",
        "\n",
        "for row in range(5):\n",
        "  for col in range(2):\n",
        "    l= rn.randint(0, len(Z))\n",
        "    ax[row, col].imshow(X[l])\n",
        "    ax[row, col].set_title(\"Flower: \"+Z[l])\n",
        "\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0YTSgVwvwBB",
        "outputId": "82192811-3522-4e29-f194-708b18c7bff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[0.24313725 0.21960784 0.32941176]\n",
            "   [0.33333333 0.31764706 0.42745098]\n",
            "   [0.34509804 0.33333333 0.45490196]\n",
            "   ...\n",
            "   [0.35294118 0.34509804 0.50588235]\n",
            "   [0.35294118 0.32156863 0.4627451 ]\n",
            "   [0.29803922 0.25490196 0.34117647]]\n",
            "\n",
            "  [[0.25882353 0.24705882 0.34509804]\n",
            "   [0.30196078 0.29019608 0.39215686]\n",
            "   [0.31764706 0.33333333 0.47843137]\n",
            "   ...\n",
            "   [0.34117647 0.30980392 0.45882353]\n",
            "   [0.30588235 0.30196078 0.42352941]\n",
            "   [0.27058824 0.23921569 0.34117647]]\n",
            "\n",
            "  [[0.27843137 0.2627451  0.40392157]\n",
            "   [0.25490196 0.2627451  0.40784314]\n",
            "   [0.34117647 0.35294118 0.51372549]\n",
            "   ...\n",
            "   [0.34117647 0.32156863 0.44313725]\n",
            "   [0.38823529 0.34901961 0.47843137]\n",
            "   [0.31764706 0.28627451 0.38039216]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3372549  0.50980392 0.38431373]\n",
            "   [0.47843137 0.6        0.47058824]\n",
            "   [0.38431373 0.50980392 0.36862745]\n",
            "   ...\n",
            "   [0.20784314 0.16862745 0.20392157]\n",
            "   [0.18039216 0.16862745 0.23137255]\n",
            "   [0.18823529 0.14901961 0.23137255]]\n",
            "\n",
            "  [[0.30588235 0.49019608 0.35686275]\n",
            "   [0.4745098  0.60392157 0.4745098 ]\n",
            "   [0.18823529 0.43137255 0.28627451]\n",
            "   ...\n",
            "   [0.05490196 0.0627451  0.0627451 ]\n",
            "   [0.16470588 0.22352941 0.22745098]\n",
            "   [0.44313725 0.31764706 0.34901961]]\n",
            "\n",
            "  [[0.29019608 0.48627451 0.35294118]\n",
            "   [0.44705882 0.59215686 0.45882353]\n",
            "   [0.18039216 0.43921569 0.28235294]\n",
            "   ...\n",
            "   [0.11372549 0.07843137 0.11764706]\n",
            "   [0.22352941 0.16470588 0.18431373]\n",
            "   [0.2627451  0.24705882 0.27843137]]]\n",
            "\n",
            "\n",
            " [[[0.87843137 0.84705882 0.84313725]\n",
            "   [0.8627451  0.83529412 0.81960784]\n",
            "   [0.88627451 0.85098039 0.83921569]\n",
            "   ...\n",
            "   [0.05490196 0.03529412 0.03529412]\n",
            "   [0.06666667 0.04313725 0.04705882]\n",
            "   [0.04705882 0.02745098 0.03137255]]\n",
            "\n",
            "  [[0.88235294 0.85098039 0.84705882]\n",
            "   [0.90588235 0.8745098  0.86666667]\n",
            "   [0.87843137 0.84705882 0.84313725]\n",
            "   ...\n",
            "   [0.05882353 0.04705882 0.05098039]\n",
            "   [0.05098039 0.02745098 0.03529412]\n",
            "   [0.04705882 0.03137255 0.02745098]]\n",
            "\n",
            "  [[0.92156863 0.89019608 0.89019608]\n",
            "   [0.90588235 0.8745098  0.8627451 ]\n",
            "   [0.89803922 0.8745098  0.87058824]\n",
            "   ...\n",
            "   [0.0627451  0.05490196 0.05490196]\n",
            "   [0.05098039 0.02745098 0.03529412]\n",
            "   [0.03137255 0.01568627 0.01176471]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.71372549 0.69019608 0.69411765]\n",
            "   [0.7372549  0.71764706 0.72156863]\n",
            "   [0.79215686 0.77647059 0.76078431]\n",
            "   ...\n",
            "   [0.54117647 0.60392157 0.62745098]\n",
            "   [0.62352941 0.6627451  0.68235294]\n",
            "   [0.65098039 0.6745098  0.71372549]]\n",
            "\n",
            "  [[0.86666667 0.84705882 0.83137255]\n",
            "   [0.89411765 0.87843137 0.84705882]\n",
            "   [0.90196078 0.88627451 0.86666667]\n",
            "   ...\n",
            "   [0.53333333 0.6        0.63529412]\n",
            "   [0.62352941 0.67058824 0.70196078]\n",
            "   [0.63529412 0.66666667 0.70588235]]\n",
            "\n",
            "  [[0.92941176 0.90588235 0.89803922]\n",
            "   [0.92156863 0.89411765 0.88235294]\n",
            "   [0.94509804 0.9254902  0.90980392]\n",
            "   ...\n",
            "   [0.54117647 0.59607843 0.63137255]\n",
            "   [0.60784314 0.67058824 0.70196078]\n",
            "   [0.62745098 0.65098039 0.70196078]]]\n",
            "\n",
            "\n",
            " [[[0.0745098  0.20784314 0.32156863]\n",
            "   [0.07843137 0.21176471 0.32941176]\n",
            "   [0.09019608 0.22745098 0.34117647]\n",
            "   ...\n",
            "   [0.         0.14509804 0.23529412]\n",
            "   [0.         0.14509804 0.23529412]\n",
            "   [0.         0.14509804 0.23529412]]\n",
            "\n",
            "  [[0.0745098  0.20784314 0.32156863]\n",
            "   [0.08235294 0.21176471 0.32941176]\n",
            "   [0.09019608 0.22745098 0.34117647]\n",
            "   ...\n",
            "   [0.         0.15294118 0.23921569]\n",
            "   [0.         0.14901961 0.23529412]\n",
            "   [0.         0.14901961 0.23529412]]\n",
            "\n",
            "  [[0.0745098  0.20784314 0.32156863]\n",
            "   [0.0745098  0.21568627 0.3254902 ]\n",
            "   [0.08235294 0.22745098 0.3372549 ]\n",
            "   ...\n",
            "   [0.         0.15294118 0.23921569]\n",
            "   [0.00392157 0.15686275 0.24313725]\n",
            "   [0.00392157 0.15686275 0.24313725]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.         0.05882353 0.12156863]\n",
            "   [0.         0.0627451  0.12941176]\n",
            "   [0.00784314 0.07058824 0.1372549 ]\n",
            "   ...\n",
            "   [0.00392157 0.00784314 0.00784314]\n",
            "   [0.00784314 0.00784314 0.00784314]\n",
            "   [0.00392157 0.00392157 0.00392157]]\n",
            "\n",
            "  [[0.         0.05882353 0.12156863]\n",
            "   [0.00392157 0.06666667 0.12941176]\n",
            "   [0.00784314 0.0745098  0.14117647]\n",
            "   ...\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]]\n",
            "\n",
            "  [[0.00392157 0.0627451  0.1254902 ]\n",
            "   [0.00392157 0.06666667 0.13333333]\n",
            "   [0.01176471 0.0745098  0.14117647]\n",
            "   ...\n",
            "   [0.         0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.         0.         0.        ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.11764706 0.15294118 0.11372549]\n",
            "   [0.14901961 0.14509804 0.12156863]\n",
            "   [0.12941176 0.14509804 0.1254902 ]\n",
            "   ...\n",
            "   [0.69019608 0.75294118 0.77647059]\n",
            "   [0.69411765 0.74901961 0.77254902]\n",
            "   [0.69411765 0.74901961 0.77254902]]\n",
            "\n",
            "  [[0.16078431 0.2        0.16862745]\n",
            "   [0.1372549  0.14117647 0.11764706]\n",
            "   [0.14901961 0.16078431 0.1372549 ]\n",
            "   ...\n",
            "   [0.69019608 0.75294118 0.77647059]\n",
            "   [0.69411765 0.75294118 0.77647059]\n",
            "   [0.69803922 0.75294118 0.77647059]]\n",
            "\n",
            "  [[0.11372549 0.14117647 0.12156863]\n",
            "   [0.14117647 0.15294118 0.12941176]\n",
            "   [0.1372549  0.15294118 0.12941176]\n",
            "   ...\n",
            "   [0.69019608 0.75294118 0.77647059]\n",
            "   [0.69019608 0.75294118 0.77647059]\n",
            "   [0.69411765 0.75686275 0.77647059]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.38039216 0.37254902 0.3372549 ]\n",
            "   [0.37647059 0.36862745 0.3372549 ]\n",
            "   [0.40392157 0.40392157 0.38039216]\n",
            "   ...\n",
            "   [0.12156863 0.14117647 0.14901961]\n",
            "   [0.09803922 0.13333333 0.1372549 ]\n",
            "   [0.11372549 0.17647059 0.18431373]]\n",
            "\n",
            "  [[0.38039216 0.36862745 0.3372549 ]\n",
            "   [0.37647059 0.36862745 0.34509804]\n",
            "   [0.40392157 0.40392157 0.38039216]\n",
            "   ...\n",
            "   [0.08235294 0.10588235 0.10196078]\n",
            "   [0.0745098  0.09803922 0.10196078]\n",
            "   [0.07058824 0.10196078 0.11372549]]\n",
            "\n",
            "  [[0.37254902 0.36862745 0.34509804]\n",
            "   [0.37254902 0.36078431 0.34117647]\n",
            "   [0.40784314 0.39607843 0.37647059]\n",
            "   ...\n",
            "   [0.01176471 0.03137255 0.02745098]\n",
            "   [0.02745098 0.04313725 0.04313725]\n",
            "   [0.05098039 0.07058824 0.06666667]]]\n",
            "\n",
            "\n",
            " [[[0.05882353 0.18823529 0.26666667]\n",
            "   [0.04705882 0.16470588 0.25098039]\n",
            "   [0.03921569 0.16470588 0.23137255]\n",
            "   ...\n",
            "   [0.01176471 0.03137255 0.02745098]\n",
            "   [0.01176471 0.03137255 0.02352941]\n",
            "   [0.01176471 0.03137255 0.02352941]]\n",
            "\n",
            "  [[0.0627451  0.20392157 0.30196078]\n",
            "   [0.06666667 0.18823529 0.29019608]\n",
            "   [0.05490196 0.18823529 0.28235294]\n",
            "   ...\n",
            "   [0.00784314 0.02745098 0.02352941]\n",
            "   [0.01176471 0.03137255 0.02352941]\n",
            "   [0.01176471 0.03137255 0.02352941]]\n",
            "\n",
            "  [[0.05490196 0.20784314 0.31372549]\n",
            "   [0.0627451  0.2        0.3254902 ]\n",
            "   [0.0627451  0.19607843 0.3254902 ]\n",
            "   ...\n",
            "   [0.00784314 0.03137255 0.02745098]\n",
            "   [0.01176471 0.03137255 0.02352941]\n",
            "   [0.01176471 0.03137255 0.02352941]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.05490196 0.08627451 0.1254902 ]\n",
            "   [0.04705882 0.07843137 0.11372549]\n",
            "   [0.04705882 0.0745098  0.10980392]\n",
            "   ...\n",
            "   [0.40392157 0.58431373 0.70588235]\n",
            "   [0.42352941 0.57254902 0.69803922]\n",
            "   [0.43529412 0.58039216 0.71372549]]\n",
            "\n",
            "  [[0.03921569 0.0745098  0.11372549]\n",
            "   [0.03921569 0.0745098  0.10980392]\n",
            "   [0.03137255 0.0745098  0.10588235]\n",
            "   ...\n",
            "   [0.38823529 0.58039216 0.69019608]\n",
            "   [0.41960784 0.56862745 0.69411765]\n",
            "   [0.42352941 0.57254902 0.70196078]]\n",
            "\n",
            "  [[0.03921569 0.06666667 0.10196078]\n",
            "   [0.04313725 0.07058824 0.10588235]\n",
            "   [0.03921569 0.07058824 0.09803922]\n",
            "   ...\n",
            "   [0.37647059 0.55294118 0.69019608]\n",
            "   [0.40392157 0.55294118 0.67843137]\n",
            "   [0.32941176 0.55294118 0.60784314]]]\n",
            "\n",
            "\n",
            " [[[0.67843137 0.83137255 0.88235294]\n",
            "   [0.62745098 0.8        0.84705882]\n",
            "   [0.4        0.67058824 0.75294118]\n",
            "   ...\n",
            "   [0.65882353 0.68235294 0.79607843]\n",
            "   [0.65490196 0.68627451 0.8       ]\n",
            "   [0.65882353 0.68235294 0.8       ]]\n",
            "\n",
            "  [[0.68627451 0.83529412 0.89019608]\n",
            "   [0.63921569 0.8        0.84313725]\n",
            "   [0.41568627 0.6745098  0.75294118]\n",
            "   ...\n",
            "   [0.6745098  0.69019608 0.80392157]\n",
            "   [0.6627451  0.68627451 0.8       ]\n",
            "   [0.67058824 0.68627451 0.8       ]]\n",
            "\n",
            "  [[0.68627451 0.83529412 0.89019608]\n",
            "   [0.64313725 0.80392157 0.84313725]\n",
            "   [0.45882353 0.69411765 0.74901961]\n",
            "   ...\n",
            "   [0.66666667 0.69019608 0.80392157]\n",
            "   [0.66666667 0.69019608 0.80392157]\n",
            "   [0.6627451  0.68627451 0.8       ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.29803922 0.26666667 0.38039216]\n",
            "   [0.26666667 0.24705882 0.34901961]\n",
            "   [0.29019608 0.24313725 0.3372549 ]\n",
            "   ...\n",
            "   [0.31372549 0.38431373 0.56862745]\n",
            "   [0.29019608 0.38039216 0.56078431]\n",
            "   [0.30196078 0.37254902 0.55686275]]\n",
            "\n",
            "  [[0.28627451 0.26666667 0.38039216]\n",
            "   [0.26666667 0.24313725 0.34509804]\n",
            "   [0.27843137 0.24705882 0.34901961]\n",
            "   ...\n",
            "   [0.31372549 0.38039216 0.56078431]\n",
            "   [0.29803922 0.37647059 0.56078431]\n",
            "   [0.30588235 0.38039216 0.55686275]]\n",
            "\n",
            "  [[0.2627451  0.25490196 0.37254902]\n",
            "   [0.2745098  0.25490196 0.36078431]\n",
            "   [0.29019608 0.27843137 0.39215686]\n",
            "   ...\n",
            "   [0.31372549 0.37647059 0.55686275]\n",
            "   [0.30196078 0.38039216 0.56078431]\n",
            "   [0.31372549 0.37647059 0.55294118]]]]\n"
          ]
        }
      ],
      "source": [
        "#encoding all the flower names\n",
        "le=LabelEncoder()\n",
        "y=le.fit_transform(Z)\n",
        "y=to_categorical(y,5)\n",
        "x=np.array(X)\n",
        "x=x/255\n",
        "# x-->contains all the image array\n",
        "# y-->contains all the image names\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbEOUzs41QrI",
        "outputId": "251b3b83-1d08-4b18-8de3-35640bc97f4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnO0cMjZ1adA"
      },
      "outputs": [],
      "source": [
        "# spliting to test and train dataset\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VBf2FdN3HXB",
        "outputId": "7c2a8b0f-5296-4c7d-9623-5ebbd5dbe847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Sunflower', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'Daisy', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'dandelion', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose', 'Rose']\n"
          ]
        }
      ],
      "source": [
        "print(Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htVqNJa_7Bkz"
      },
      "outputs": [],
      "source": [
        "#set random seed\n",
        "np.random.seed(42)\n",
        "rn.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCgHvh079ENg"
      },
      "outputs": [],
      "source": [
        "#building the model\n",
        "model=Sequential()\n",
        "model.add(Conv2D(filters=32,kernel_size=(5,5),padding='same',activation='relu',input_shape=(150,150,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#1 2 3 4\n",
        "#5 6 7 8\n",
        "#9 10 11 12\n",
        "#13 14 15 16\n",
        "\n",
        "#MaxPooling --->2x2\n",
        "#6 8\n",
        "#14 16\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=96,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5,activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H7ayL3ICtti"
      },
      "outputs": [],
      "source": [
        "#adjust the learning rate\n",
        "batch_size=128\n",
        "epochs=50\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr=ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lhtyi_yFMx8"
      },
      "outputs": [],
      "source": [
        "#to prevent overfitting\n",
        "datagen=ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "   samplewise_center= False,\n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False,\n",
        "    zca_whitening=False,\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False)\n",
        "datagen.fit(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEm5pM4pGkHV",
        "outputId": "9336aa05-45a4-43c9-dfcf-3aa73348133b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "#compile keras model\n",
        "model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIR_cZrnHok_",
        "outputId": "6184bf1a-a54c-4e23-d499-1b3e314d769f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 150, 150, 32)      2432      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 75, 75, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 75, 75, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 37, 37, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 37, 37, 96)        55392     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 18, 18, 96)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 31104)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               15925760  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16004645 (61.05 MB)\n",
            "Trainable params: 16004645 (61.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpIr8xcUI4Xf",
        "outputId": "97ff53e1-3bd3-4791-baf8-700a2feeeda4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-52-1d9c612dc1fb>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  History=model.fit_generator(datagen.flow(x_train,y_train,batch_size=batch_size),epochs=epochs,validation_data=(x_test,y_test),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "11/11 [==============================] - 97s 8s/step - loss: 1.7532 - accuracy: 0.4035 - val_loss: 1.0775 - val_accuracy: 0.5056\n",
            "Epoch 2/50\n",
            "11/11 [==============================] - 97s 9s/step - loss: 1.0430 - accuracy: 0.5610 - val_loss: 0.9166 - val_accuracy: 0.6517\n",
            "Epoch 3/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.9053 - accuracy: 0.6559 - val_loss: 0.8882 - val_accuracy: 0.6292\n",
            "Epoch 4/50\n",
            "11/11 [==============================] - 95s 9s/step - loss: 0.8642 - accuracy: 0.6597 - val_loss: 0.7754 - val_accuracy: 0.6994\n",
            "Epoch 5/50\n",
            "11/11 [==============================] - 92s 8s/step - loss: 0.8909 - accuracy: 0.6381 - val_loss: 0.7854 - val_accuracy: 0.6798\n",
            "Epoch 6/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.8163 - accuracy: 0.6829 - val_loss: 0.7195 - val_accuracy: 0.7303\n",
            "Epoch 7/50\n",
            "11/11 [==============================] - 93s 9s/step - loss: 0.7483 - accuracy: 0.7052 - val_loss: 0.6682 - val_accuracy: 0.7444\n",
            "Epoch 8/50\n",
            "11/11 [==============================] - 94s 8s/step - loss: 0.7368 - accuracy: 0.7153 - val_loss: 0.7036 - val_accuracy: 0.7416\n",
            "Epoch 9/50\n",
            "11/11 [==============================] - 94s 9s/step - loss: 0.6911 - accuracy: 0.7315 - val_loss: 0.6375 - val_accuracy: 0.7669\n",
            "Epoch 10/50\n",
            "11/11 [==============================] - 93s 9s/step - loss: 0.6505 - accuracy: 0.7569 - val_loss: 0.5620 - val_accuracy: 0.7753\n",
            "Epoch 11/50\n",
            "11/11 [==============================] - 94s 9s/step - loss: 0.6065 - accuracy: 0.7716 - val_loss: 0.5945 - val_accuracy: 0.7809\n",
            "Epoch 12/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.6006 - accuracy: 0.7739 - val_loss: 0.5813 - val_accuracy: 0.7781\n",
            "Epoch 13/50\n",
            "11/11 [==============================] - 92s 8s/step - loss: 0.6615 - accuracy: 0.7485 - val_loss: 0.5591 - val_accuracy: 0.7949\n",
            "Epoch 14/50\n",
            "11/11 [==============================] - 88s 8s/step - loss: 0.6078 - accuracy: 0.7716 - val_loss: 0.5846 - val_accuracy: 0.7753\n",
            "Epoch 15/50\n",
            "11/11 [==============================] - 94s 8s/step - loss: 0.6063 - accuracy: 0.7755 - val_loss: 0.5329 - val_accuracy: 0.7949\n",
            "Epoch 16/50\n",
            "11/11 [==============================] - 88s 8s/step - loss: 0.5591 - accuracy: 0.7971 - val_loss: 0.5420 - val_accuracy: 0.7893\n",
            "Epoch 17/50\n",
            "11/11 [==============================] - 88s 8s/step - loss: 0.5400 - accuracy: 0.7909 - val_loss: 0.6917 - val_accuracy: 0.7331\n",
            "Epoch 18/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.5416 - accuracy: 0.7894 - val_loss: 0.5089 - val_accuracy: 0.8034\n",
            "Epoch 19/50\n",
            "11/11 [==============================] - 87s 8s/step - loss: 0.5117 - accuracy: 0.8032 - val_loss: 0.5137 - val_accuracy: 0.7978\n",
            "Epoch 20/50\n",
            "11/11 [==============================] - 95s 8s/step - loss: 0.4879 - accuracy: 0.8164 - val_loss: 0.5688 - val_accuracy: 0.7921\n",
            "Epoch 21/50\n",
            "11/11 [==============================] - 92s 8s/step - loss: 0.4809 - accuracy: 0.8102 - val_loss: 0.5170 - val_accuracy: 0.8062\n",
            "Epoch 22/50\n",
            "11/11 [==============================] - 90s 8s/step - loss: 0.4657 - accuracy: 0.8264 - val_loss: 0.6207 - val_accuracy: 0.7781\n",
            "Epoch 23/50\n",
            "11/11 [==============================] - 93s 9s/step - loss: 0.5194 - accuracy: 0.7932 - val_loss: 0.4869 - val_accuracy: 0.7978\n",
            "Epoch 24/50\n",
            "11/11 [==============================] - 91s 8s/step - loss: 0.4724 - accuracy: 0.8256 - val_loss: 0.5140 - val_accuracy: 0.7865\n",
            "Epoch 25/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.4975 - accuracy: 0.8194 - val_loss: 0.4943 - val_accuracy: 0.7893\n",
            "Epoch 26/50\n",
            "11/11 [==============================] - 101s 9s/step - loss: 0.4610 - accuracy: 0.8196 - val_loss: 0.4683 - val_accuracy: 0.8034\n",
            "Epoch 27/50\n",
            "11/11 [==============================] - 95s 9s/step - loss: 0.4209 - accuracy: 0.8441 - val_loss: 0.4743 - val_accuracy: 0.7921\n",
            "Epoch 28/50\n",
            "11/11 [==============================] - 88s 8s/step - loss: 0.4490 - accuracy: 0.8287 - val_loss: 0.5406 - val_accuracy: 0.7865\n",
            "Epoch 29/50\n",
            "11/11 [==============================] - 90s 8s/step - loss: 0.4058 - accuracy: 0.8418 - val_loss: 0.5509 - val_accuracy: 0.7725\n",
            "Epoch 30/50\n",
            "11/11 [==============================] - 92s 8s/step - loss: 0.4431 - accuracy: 0.8310 - val_loss: 0.4944 - val_accuracy: 0.8174\n",
            "Epoch 31/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.4129 - accuracy: 0.8333 - val_loss: 0.4791 - val_accuracy: 0.8034\n",
            "Epoch 32/50\n",
            "11/11 [==============================] - 95s 9s/step - loss: 0.3983 - accuracy: 0.8519 - val_loss: 0.4621 - val_accuracy: 0.8146\n",
            "Epoch 33/50\n",
            "11/11 [==============================] - 89s 8s/step - loss: 0.4169 - accuracy: 0.8356 - val_loss: 0.4543 - val_accuracy: 0.8062\n",
            "Epoch 34/50\n",
            "11/11 [==============================] - 94s 9s/step - loss: 0.3870 - accuracy: 0.8472 - val_loss: 0.4474 - val_accuracy: 0.8174\n",
            "Epoch 35/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.4186 - accuracy: 0.8403 - val_loss: 0.5607 - val_accuracy: 0.8006\n",
            "Epoch 36/50\n",
            "11/11 [==============================] - 94s 8s/step - loss: 0.3884 - accuracy: 0.8495 - val_loss: 0.4531 - val_accuracy: 0.8399\n",
            "Epoch 37/50\n",
            "11/11 [==============================] - 92s 8s/step - loss: 0.4042 - accuracy: 0.8465 - val_loss: 0.4717 - val_accuracy: 0.8006\n",
            "Epoch 38/50\n",
            "11/11 [==============================] - 95s 8s/step - loss: 0.4067 - accuracy: 0.8488 - val_loss: 0.5172 - val_accuracy: 0.8006\n",
            "Epoch 39/50\n",
            "11/11 [==============================] - 91s 8s/step - loss: 0.3836 - accuracy: 0.8457 - val_loss: 0.5292 - val_accuracy: 0.7949\n",
            "Epoch 40/50\n",
            "11/11 [==============================] - 94s 9s/step - loss: 0.3725 - accuracy: 0.8657 - val_loss: 0.5925 - val_accuracy: 0.7753\n",
            "Epoch 41/50\n",
            "11/11 [==============================] - 93s 8s/step - loss: 0.3541 - accuracy: 0.8627 - val_loss: 0.4124 - val_accuracy: 0.8427\n",
            "Epoch 42/50\n",
            "11/11 [==============================] - 95s 9s/step - loss: 0.3575 - accuracy: 0.8596 - val_loss: 0.4452 - val_accuracy: 0.8315\n",
            "Epoch 43/50\n",
            "11/11 [==============================] - 87s 8s/step - loss: 0.3215 - accuracy: 0.8781 - val_loss: 0.4519 - val_accuracy: 0.8455\n",
            "Epoch 44/50\n",
            "11/11 [==============================] - 101s 9s/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.4479 - val_accuracy: 0.8399\n",
            "Epoch 45/50\n",
            "11/11 [==============================] - 87s 8s/step - loss: 0.3258 - accuracy: 0.8804 - val_loss: 0.4313 - val_accuracy: 0.8371\n",
            "Epoch 46/50\n",
            "11/11 [==============================] - 101s 9s/step - loss: 0.2865 - accuracy: 0.8920 - val_loss: 0.4067 - val_accuracy: 0.8455\n",
            "Epoch 47/50\n",
            "11/11 [==============================] - 92s 8s/step - loss: 0.2986 - accuracy: 0.8904 - val_loss: 0.5337 - val_accuracy: 0.8090\n",
            "Epoch 48/50\n",
            "11/11 [==============================] - 89s 8s/step - loss: 0.3254 - accuracy: 0.8789 - val_loss: 0.4536 - val_accuracy: 0.7949\n",
            "Epoch 49/50\n",
            "11/11 [==============================] - 93s 9s/step - loss: 0.3184 - accuracy: 0.8781 - val_loss: 0.4651 - val_accuracy: 0.8287\n",
            "Epoch 50/50\n",
            "11/11 [==============================] - 96s 8s/step - loss: 0.3534 - accuracy: 0.8611 - val_loss: 0.3832 - val_accuracy: 0.8483\n"
          ]
        }
      ],
      "source": [
        "#fitting on training and making predictions\n",
        "History=model.fit_generator(datagen.flow(x_train,y_train,batch_size=batch_size),epochs=epochs,validation_data=(x_test,y_test),\n",
        "                            verbose=1,steps_per_epoch=x_train.shape[0]//batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FdQOmH1JMXM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "d100890b-537a-4a75-ca93-5ef989c86e25"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'History' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3f06d7cdab03>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plt.plot(History.history['acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'History' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XMTb9soAmeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}